{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Combine spatial and temporal processing to emaphsize subtle spatial changes.\n",
        "1. take standard video sequences as input and decompose it into different spatial frequency band using laplacian pyramid\n",
        "2. take the sequence of pixel values over time and apply a temporal bandpass filter to extract the frequency band of interest\n",
        "3. resulting signal is amplified and added back to the frames\n",
        "4. collapse pyramid to generate output video"
      ],
      "metadata": {
        "id": "R3Gpc3_5PQ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import convolve2d\n",
        "from scipy import ndimage\n",
        "import skimage as sk\n",
        "from skimage import io, color\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.signal\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-7WSclSFbniZ",
        "outputId": "9bf13d86-2a9c-4a13-b7db-e96a65558efb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baby_path = \"/content/drive/MyDrive/cs180/Final Projects/Video Magnification/baby.mp4\"\n",
        "face_path = \"/content/drive/MyDrive/cs180/Final Projects/Video Magnification/face.mp4\"\n",
        "engine_path = \"/content/drive/MyDrive/cs180/Final Projects/Video Magnification/engine.mp4\""
      ],
      "metadata": {
        "id": "3FwZ-UVRjDa9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Miscellaneous"
      ],
      "metadata": {
        "id": "T-iOHvE3-euZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb_to_yiq(rgb_image):\n",
        "  transform_matrix = np.array([[0.299, 0.587, 0.114],\n",
        "                              [0.596, -0.275, -0.321],\n",
        "                              [0.212, -0.523, 0.311]])\n",
        "  reshaped_rgb = rgb_image.reshape((-1, 3))\n",
        "  yiq = reshaped_rgb @ transform_matrix.T\n",
        "  yiq_image = yiq.reshape(rgb_image.shape)\n",
        "  return yiq_image\n",
        "\n",
        "def yiq_to_rgb(yiq_image):\n",
        "  inverse_transform_matrix = np.array([[1, 0.956, 0.621],\n",
        "                              [1, -0.272, -0.647],\n",
        "                              [1, -1.106, 1.703]])\n",
        "  reshaped_yiq = yiq_image.reshape((-1,3))\n",
        "  rgb = reshaped_yiq @ inverse_transform_matrix.T\n",
        "  rgb_image = np.clip(rgb, 0, 255).reshape(yiq_image.shape).astype(np.uint8)\n",
        "  return rgb_image\n",
        "\n",
        "def convert_frames_to_yiq(rgb_frames):\n",
        "  yiq_frames = []\n",
        "  for frame in rgb_frames:\n",
        "    yiq_frames.append(rgb_to_yiq(frame))\n",
        "  return yiq_frames\n",
        "def convert_frames_to_rgb(yiq_frames):\n",
        "  rgb_frames = []\n",
        "  for frame in yiq_frames:\n",
        "    rgb_frames.append(yiq_to_rgb(frame))\n",
        "  return rgb_frames"
      ],
      "metadata": {
        "id": "KBm9FBoE6UDl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video(file_path):\n",
        "  video = open(file_path, 'rb').read()\n",
        "  video_encoded = b64encode(video).decode('ascii')\n",
        "  video_tag = f'<video controls alt=\"output video\" src=\"data:video/mp4;base64,{video_encoded}\">'\n",
        "  return HTML(video_tag)\n",
        "\n"
      ],
      "metadata": {
        "id": "loXAuTiajYxl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplacian pyramid"
      ],
      "metadata": {
        "id": "LLD2oLDuR6gy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_Q17-T8zPOMG"
      },
      "outputs": [],
      "source": [
        "def generate_gaussian_pyramid(image, num_levels, kernel_size, sigma):\n",
        "  gaussian_pyramid = [image]\n",
        "  for _ in range(1, num_levels):\n",
        "    image = cv2.pyrDown(image)\n",
        "    gaussian_pyramid.append(image)\n",
        "  return gaussian_pyramid\n",
        "\n",
        "def generate_laplacian_pyramid(image, num_levels, kernel_size=80, sigma=1):\n",
        "  gaussian_pyramid = generate_gaussian_pyramid(image, num_levels, kernel_size, sigma)\n",
        "  laplacian_pyramid = []\n",
        "  for i in range(len(gaussian_pyramid) - 1):\n",
        "    size = (gaussian_pyramid[i].shape[1],gaussian_pyramid[i].shape[0])\n",
        "    expanded = cv2.pyrUp(gaussian_pyramid[i + 1], dstsize=size)\n",
        "    layer = cv2.subtract(gaussian_pyramid[i], expanded)\n",
        "    laplacian_pyramid.append(layer)\n",
        "  laplacian_pyramid.append(gaussian_pyramid[-1])\n",
        "  return laplacian_pyramid\n",
        "\n",
        "def collapse_laplacian_pyramid(laplacian_pyramid):\n",
        "  reconstructed_image = laplacian_pyramid[-1]\n",
        "  for level in reversed(laplacian_pyramid[:-1]):\n",
        "    size = (level.shape[1], level.shape[0])  # (width, height)\n",
        "    reconstructed_image = cv2.pyrUp(reconstructed_image, dstsize=size)\n",
        "    reconstructed_image = cv2.add(reconstructed_image, level)\n",
        "  return reconstructed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Temporal filtering"
      ],
      "metadata": {
        "id": "RTr9eeX2e7jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#=============================================\n",
        "# extract time series\n",
        "#=============================================\n",
        "def extract_time_series(frames, num_levels):\n",
        "  # laplacian pyramids for each frame in frames\n",
        "  laplacian_pyramids = [generate_laplacian_pyramid(frame, num_levels) for frame in frames]\n",
        "\n",
        "  shapes_per_layer = [layer.shape for layer in laplacian_pyramids[0]]\n",
        "  time_series = [np.zeros((shape[0], shape[1], shape[2], len(frames))) for shape in shapes_per_layer]\n",
        "\n",
        "  for laplacian_index, laplacian in enumerate(laplacian_pyramids):\n",
        "    for level_index, layer in enumerate(laplacian):\n",
        "      time_series[level_index][:, :, :, laplacian_index] = layer\n",
        "  return time_series, laplacian_pyramids\n",
        "\n",
        "#=============================================\n",
        "# filter time series using freqz\n",
        "#=============================================\n",
        "\n",
        "\n",
        "# Band-pass Filtering, filter time series before convert to FFT\n",
        "def filter_timeseries(time_series, lowcut = 0.2, highcut = 1.5, fs = 60, n = 6):\n",
        "  '''\n",
        "  input: timeseries: a list of NumPy arrays,each has shape (height, width, number of color channels, number of frames)\n",
        "        lowcut: low cut frequency in Hz\n",
        "        highcut: high cut frequency in Hz\n",
        "        fs: sampling frequency\n",
        "        n: sample order\n",
        "\n",
        "  output: filtered time series\n",
        "  '''\n",
        "  filtered_time_series_per_level = []\n",
        "  for ts in time_series:\n",
        "    # create a Butterworth band-pass filter; coefficients of the filter's transfer function\n",
        "    b, a = scipy.signal.butter(n, [lowcut, highcut], btype='band', fs=fs)\n",
        "\n",
        "    # convert to frequency domain\n",
        "    fft_series = np.fft.fft(ts, axis = 3)\n",
        "    # get the frequency components of the filter\n",
        "    # w: frequencies, h: frequency responses at each frequency in w\n",
        "    w, h = scipy.signal.freqz(b, a, worN=fft_series.shape[3])\n",
        "\n",
        "    # apply filter's effect\n",
        "    filtered_fft = fft_series * h\n",
        "\n",
        "    # fft -> time series\n",
        "    filtered_time_series = np.fft.ifft(filtered_fft)\n",
        "    filtered_time_series_per_level.append(filtered_time_series)\n",
        "\n",
        "  return filtered_time_series_per_level"
      ],
      "metadata": {
        "id": "g5KEtlQTbuEn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image reconstruction"
      ],
      "metadata": {
        "id": "_BTMB5ztfBR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def amplify_changes(time_domain_series, amplification_factor = 50):\n",
        "  return [ts * amplification_factor for ts in time_domain_series]\n",
        "\n",
        "\n",
        "def integrate_amplified_changes(laplacian_pyramids, amplified_time_series):\n",
        "  '''\n",
        "  laplacian_pyramids : num_framaes, num_pyramid_layers, y,x,c\n",
        "  amplified_time_series: num_pyramid_layers, y,x,c, num_frames\n",
        "  '''\n",
        "  modified_pyramids = []\n",
        "  # iterate through each laplacian pyramid for each frame\n",
        "  for pyramid_index, pyramid in enumerate(laplacian_pyramids):\n",
        "    # construct a new pyramid for that frame\n",
        "    curr_pyramid = []\n",
        "    # for each layer of this pyramid\n",
        "    for level_index, layer in enumerate(pyramid):\n",
        "      # convert layer to real number\n",
        "      layer = layer\n",
        "      # this is the amplified layer at given frame\n",
        "      amplified_layer = amplified_time_series[level_index][:,:,:,pyramid_index] # (y,x,c, num_frames)\n",
        "      assert amplified_layer.shape == layer.shape\n",
        "      modified_layer = amplified_layer + layer\n",
        "      modified_layer = np.real(modified_layer).astype(np.float32)\n",
        "      curr_pyramid.append(modified_layer)\n",
        "    modified_pyramids.append(curr_pyramid)\n",
        "  return modified_pyramids\n",
        "\n",
        "def collapse_laplacian_pyramid(laplacian_pyramid):\n",
        "  reconstructed_image = laplacian_pyramid[-1]\n",
        "  for level in reversed(laplacian_pyramid[:-1]):\n",
        "    size = (level.shape[1], level.shape[0])  # (width, height)\n",
        "    reconstructed_image = cv2.pyrUp(reconstructed_image, dstsize=size)\n",
        "    reconstructed_image = cv2.add(reconstructed_image, level)\n",
        "  return reconstructed_image\n",
        "\n",
        "def reconstruct_frames_from_pyramids(modified_pyramids):\n",
        "  reconstructed_frames = []\n",
        "  for pyramid in modified_pyramids:\n",
        "    frame = collapse_laplacian_pyramid(pyramid)\n",
        "    reconstructed_frames.append(frame)\n",
        "  return reconstructed_frames\n"
      ],
      "metadata": {
        "id": "091vXQrXZKSH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "def video_magnification(video_path, output_dir, batch_size = 25):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  out = cv2.VideoWriter(output_dir, fourcc, fps, (width, height))\n",
        "\n",
        "  # Read each frame from the video\n",
        "  while True:\n",
        "\n",
        "    input_frame = []\n",
        "\n",
        "    for _ in range(batch_size):\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "          break\n",
        "      rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      input_frame.append(rgb_frame)\n",
        "\n",
        "    if not input_frame:\n",
        "      break\n",
        "\n",
        "    yiq_input_frames = convert_frames_to_yiq(input_frame)\n",
        "    time_series, laplacian_pyramids = extract_time_series(yiq_input_frames, num_levels = 8)\n",
        "    filtered_time_series_list = filter_timeseries(time_series)\n",
        "    amplified_time_series = amplify_changes(filtered_time_series_list)\n",
        "    modified_pyramids = integrate_amplified_changes(laplacian_pyramids, amplified_time_series)\n",
        "    reconstructed_frames = reconstruct_frames_from_pyramids(modified_pyramids)\n",
        "    reconstructed_frames_rgb = convert_frames_to_rgb(reconstructed_frames)\n",
        "\n",
        "    for frame in reconstructed_frames_rgb:\n",
        "      frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "      out.write(frame_bgr)\n",
        "\n",
        "\n",
        "    del input_frame\n",
        "    del yiq_input_frames\n",
        "    del reconstructed_frames_rgb\n",
        "    del reconstructed_frames\n",
        "    del time_series\n",
        "    del laplacian_pyramids\n",
        "    del filtered_time_series_list\n",
        "    del amplified_time_series\n",
        "    del modified_pyramids\n",
        "    gc.collect()\n",
        "    # break\n",
        "\n",
        "  cap.release()\n",
        "\n",
        "video_magnification(engine_path, '/content/engine_color.mp4')"
      ],
      "metadata": {
        "id": "ZdXXlOlyfHrj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbYDCqeTxIaF"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}