<!DOCTYPE HTML>
<html>
	<head>
		<title>CS180 Final Projects</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">


			<nav id="nav">
				<ul class="container">
					<li><a href="#nn">NN for Artistic Style</a></li>
					<li><a href="#aug">Augmented Reality</a></li>
					<li><a href="#mag">Video Magnification</a></li>
				</ul>
			</nav>


			<article id="top" class="wrapper style1">
				<div class="container">
					<div class="row">
						<div class="col-8 col-7-large col-12-medium">
							<header>
								<h1 style="margin-bottom: 10px; padding-bottom: 0px; border-bottom: 0px;">CS180 <strong>Final Projects</strong>.</h1>
								<p>Claire Ding, Thomas Davis-Mena</p>
							</header>
							<p>Below we describe our implementations of <strong><a href="#nn">A  Neural Network for Artistic Style</a></strong>, <strong><a href="#aug">Augmented Reality</a></strong>, and <strong><a href="#mag">Video Magnification</a></strong>.We hope you enjoy what we have to show!</p>

						</div>
					</div>
				</div>
			</article>

			<article id="nn" class="wrapper style2">
				<div class="container">
					<header>
						<h2>Neural Network for Artistic Style</h2>
						<p>The concept of style transfer in neural networks typically involves taking two images, a content image (e.g. a photograph) and a style image (e.g. a painting), and blending them together so that the output image looks like the content image but has the style of the style image. The algorithm we chose is VGG19, which is a pretrained model that is well suited for feature extraction. 
						We followed the instructions outlined by<a href = "https://arxiv.org/pdf/1508.06576.pdf"> A Neural Algorithm of Artistic Style</a>. 
						<br/><br/>Druing the implementation, we first pre-processed both content and style images by normalizing them to the specific standard deviation and mean required by the vgg19 model. We then extracted the content feature (conv4_2 layer) and style feature (conv1_1, conv2_1, conv3_1, conv4_1, and conv5_1 layers) from content image and style image respectively by passing them through the neural network. For content loss, we took the mean squared error between the content representation of the content image and target image. For style loss, we first defined a function to compute gram matrix of feature maps to represent texture and color information, then we took the mean squared loss of gram matrix of style image and target image.
						<br/><br/> During the optimization, we first initialized the target image into random noise. We set content loss weight and style loss weight to 5e2, 1e-5 respectively to achieve the optimal result and learning rate to 0.01 for our Adam optimizer. We trained for 30000 iterations and we extracted output result for every 100 steps. During each iteration, we normalize the target image and put it through Sigmoid layer before we extract its content and style representation to compare with content and style images. 
						<br/><br/> We observed that the style loss decreased drammatically at the beginning. We first got the gist of the style, then we gradually got the contour of the content image in our output image. Below are the graphs of weighted content loss, weighted style loss, and total loss.<br/><br/></p>
				<div class="row">
					<div class="col-4 col-6-medium col-12-small">
						<article class="box style2">
							<a href="#" class="image featured"><img src="Style Loss.png" alt="" /></a>
							<h3><a href="#">Style Loss</a></h3>
							
						</article>
					</div>
					<div class="col-4 col-6-medium col-12-small">
						<article class="box style2">
							<a href="#" class="image featured"><img src="Content Loss.png" alt="" /></a>
							<h3><a href="#">Content Loss</a></h3>
					
						</article>
					</div>
					<div class="col-4 col-6-medium col-12-small">
						<article class="box style2">
							<a href="#" class="image featured"><img src="Total Loss.png" alt="" /></a>
							<h3><a href="#">Total Loss</a></h3>
							
						</article>
					</div>
				</div>
				<p><br/><br/>We first tried combining the image of Neckarfront in Tubingen, which is used as example in the paper, and an oil painitng Gloucester Harbor. <br/><br/></p>
				<div class="row">
					<div class="col-4 col-6-medium col-12-small">
						<article class="box style2">
							<a href="#" class="image featured"><img src="Gloucester_Harbor.jpeg" alt="" /></a>
							<h3><a href="#">Style</a></h3>
							<p>Gloucester Harbor</p>
						</article>
					</div>
					<div class="col-4 col-6-medium col-12-small">
						<article class="box style2">
							<a href="#" class="image featured"><img src="testing.jpeg" alt="" /></a>
							<h3><a href="#">Content</a></h3>
							<p>Neckarfront in Tubingen, Germany</p>
						</article>
					</div>
					<div class="col-4 col-6-medium col-12-small">
						<article class="box style2">
							<a href="#" class="image featured"><img src="testing2Gluc.gif" alt="" /></a>
							<h3><a href="#">Result</a></h3>
							<p>Training Process</p>
						</article>
					</div>
				</div>
				<p><br/><br/>Then we decided to mess around with our beautiful campus and its iconic tower.<br/><br/></p>
			<div></div>
			<div class="row">
				<div class="col-4 col-6-medium col-12-small">
					<article class="box style2">
						<a href="#" class="image featured"><img src="Gloucester_Harbor.jpeg" alt="" /></a>
						<h3><a href="#">Style</a></h3>
						<p>Gloucester Harbor</p>
					</article>
				</div>
				<div class="col-4 col-6-medium col-12-small">
					<article class="box style2">
						<a href="#" class="image featured"><img src="doe-2.png" alt="" /></a>
						<h3><a href="#">Content</a></h3>
						<p>Doe Library and Sather Tower at Cal</p>
					</article>
				</div>
				<div class="col-4 col-6-medium col-12-small">
					<article class="box style2">
						<a href="#" class="image featured"><img src="Doe2Gluc.gif" alt="" /></a>
						<h3><a href="#">Result</a></h3>
						<p>Training Process</p>
					</article>
				</div>
			</div>
			<p><br/><br/>Finally, we wanted to confirm that the content representation extracted by neural network only captures its high-level content rather than its style. Therefore, this time, instead of taking a pure photogtaph as our content image, we chose the result of the previous style transfered image, the sather tower with Gloucester Harbor style, as its content image, and another oil painting, Wheat Field, as its style image, to see if the style of the content image will influence the output. <br/><br/></p>
			<div class="row">
				<div class="col-4 col-6-medium col-12-small">
					<article class="box style2">
						<a href="#" class="image featured"><img src="wheat_field.jpeg" alt="" /></a>
						<h3><a href="#">Style</a></h3>
						<p>Wheat Field</p>
					</article>
				</div>
				<div class="col-4 col-6-medium col-12-small">
					<article class="box style2">
						<a href="#" class="image featured"><img src="doe-2_glucosester_harbor.jpeg" alt="" /></a>
						<h3><a href="#">Content</a></h3>
						<p>Doe Library and Sather Tower at Cal with Gloucester Harbor style</p>
					</article>
				</div>
				<div class="col-4 col-6-medium col-12-small">
					<article class="box style2">
						<a href="#" class="image featured"><img src="doe2Gluc2wheat.gif" alt="" /></a>
						<h3><a href="#">Result</a></h3>
						<p>Training Process</p>
					</article>
				</div>
			</div>
			<p><br/><br/>As we had anticipated, it didn't!!<br/><br/></p>

					</header>

				</div>
			</article>

			<article id="aug" class="wrapper style2">
				<div class="container">
					<header>
						<h2>Augmented Reality</h2>
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="aug.gif" alt="" /></a>
								<h3><a href="#">Set-up</a></h3>
								<p>For our implementation of <a href="https://inst.eecs.berkeley.edu/~cs194-26/sp20/hw/proj5/ar.html">Poor Man's Augmented Reality</a>, the first step was setting up a box containing non-planar points. In our case, we used a box with two surfaces holding points that are roughly 6cm apart.</p>
							</article>
						</br>
						</div>
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="axis.jpg" alt="" /></a>
								<h3><a href="#">Recording Known Data</a></h3>
								<p>Subsequently, we recorded the 2D image points for every dot in the first frame and determined their relative 3D positions using the axes defined above with the origin centered around the bottom left corner. In this example, the blue, red,and green lines correspond to the x, y,and z-axes, respectively.</p>
							</article>
						</br>
						</div>
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="MOSSEtrack.gif" alt="" /></a>
								<h3><a href="#">Propogating Data Across Frames</a></h3>
								<p>After collecting these 2D image points and defining this 3D world coordinate system, the next step was to propogate the 2D image coordinates for every following frame. We chose to use an off-the-shelf tracker to find those matching image coordinates.
									After testing a few trackers, we came to a consensus on using openCV's MOSSE tracker as it had both high accuracy and speed for our purpose. The following shows both our choice of bounding boxes and point centers after applying the MOSSE tracker to the video.</p>							
								</article>
							</br>
						</div>
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="origin.gif" alt="" /></a>
								<h3><a href="#">Projecting Cube at Origin</a></h3>
								<p>Next, we used least squares to fit the previously defined 4 dimenional world coordinates to their corresponding 3 dimentional image coordinates(homogeneous coordinates) per frame. These projection matrices allowed us to easily find the corresponding image coordinate with any chosen world coordinate in a given frame. This allowed us to place a cube along any axis defined from the previous step and reconstruct a video of the cube at any angle. Above is a cube projected at the origin at half a unit size(3cm at every dimension).</p>
							</article>
						</br>
						</div>
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="onTop.gif" alt="" /></a>
								<h3><a href="#">Final Result</a></h3>
								<p>Here is our final result of the cube in an arbitrary spot on top of the box(0,1,2). The cube is 2 units(12cm) at every dimension this time around. This concludes our augmented reality implementation!</p>
							</article>
						</div>
					</header>
				</div>
			</article>

			<article id="mag" class="wrapper style2">
				<div class="container">
					<header>
						<h2>Video Magnification</h2>
						<p>Eulerian Video Magnification is a fascinating technique used to amplify subtle changes in videos that are invisible to the naked eye. It can be used to visualize minute motions or color variations, like the pulsing of blood vessels or the vibrations of objects. In this project, we followed the procedures outlined by <a src = 'http://people.csail.mit.edu/mrub/papers/vidmag.pdf'>Eulerian Video Magnification for Revealing Subtle Changes in the World</a>.<br/>
							We first extracted frames from our input video and convert each frame from RGB to YIQ (because it easily amplifies intensity and chromaticity independently of each other). Then, we decomposed each frame spatially into different spatial frequency bands using Laplacian pyramids. For each spatial band, we applied a band-pass filter temporally to extract the frequency of interest, such as the frequency of a heartbeat or breathing rate, to isolate subtle motions or color changes that we are interested in amplifying. The band-pass filter we chose is Butterworth band-pass filter. Since performing filtering in the frequency domain can be more efficient than in the time domain, we first converted this time series to the frequency domain using the Fast Fourier Transform. We then extracted frequency responses using freqz, and multiplied the FFT of the signal by the frequency response of the filter. This applies the filter's effect (both amplitude and phase alterations) to the signal in the frequency domain.
							We then amplified the temporal changes with frequency band of interest by applying amplification factors. Finally, we reconstruct the video by combining the processed layers back into a single stream and collapsed the Laplacian pyramids.</p>
					</div>
					<h1>Original</h1>
					<div class="row">
						<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="face.gif" alt="" /></a>
								<h3><a href="#">Video 1</a></h3>
								<p>Original face</p>
							</article>
						</div>
							<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="baby.gif" alt="" /></a>
								<h3><a href="#">Video 2</a></h3>
								<p>Original baby</p>
							</article>
						</div>
							<div class="col-4 col-6-medium col-12-small">
							<article class="box style2">
								<a href="#" class="image featured"><img src="engine.gif" alt="" /></a>
								<h3><a href="#">Video 3</a></h3>
								<p>Original engine</p>
							</article>
						</div>
					</div>
					<h1>Amplified</h1>
						<div class="row">
							<div class="col-4 col-6-medium col-12-small">
								<article class="box style2">
									<a href="#" class="image featured"><img src="facegif.gif" alt="" /></a>
									<h3><a href="#">Video 1</a></h3>
									<p>Amplified Blood Circulation</p>
								</article>
							</div>
							<div class="col-4 col-6-medium col-12-small">
								<article class="box style2">
									<a href="#" class="image featured"><img src="babygif.gif" alt="" /></a>
									<h3><a href="#">Video 2</a></h3>
									<p>Amplified Breathing Rate</p>
								</article>
							</div>
							<div class="col-4 col-6-medium col-12-small">
								<article class="box style2">
									<a href="#" class="image featured"><img src="newengine.gif" alt="" /></a>
									<h3><a href="#">EC #3: Video 1</a></h3>
									<p>Amplified Vibrations at 11-15hz</p>
								</article>
							</div>
							<div class="col-4 col-6-medium col-12-small">
								<article class="box style2">
									<a href="#" class="image featured"><img src="baby2.gif" alt="" /></a>
									<h3><a href="#">EC #3: Video 2</a></h3>
									<p>Amplified Vibrations at 11-15hz</p>
								</article>
							</div>
							<div class="col-4 col-6-medium col-12-small">
								<article class="box style2">
									<a href="#" class="image featured"><img src="face2.gif" alt="" /></a>
									<h3><a href="#">EC #3: Video 3</a></h3>
									<p>Amplified Vibrations at 11-15hz</p>
								</article>
							</div>
							<p>Note on Extra Credit #3 content, the first set of frequences was determined by the idle rpm of a typical honda crv which is around 750rpm. The result was a much more vibrant engine than the human eye can perceive. All the shakes are likely the internal combustion causing a much higher set of vibrational frequencies. For the other two gifs in the first set, it was quite surprising to see these similar frequencies expressed in the tiny inperceivalbe movements of the eye.</p>
						</div>
						<br/>
					</header>
				</div>
			</article>

			<article id="contact" class="wrapper style4">
				<div class="container medium">
					<footer>
						<ul id="copyright">
							<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>
				</div>
			</article>

			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>